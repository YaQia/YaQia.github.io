---
outline: deep
---

# PyTorché‡Œçš„æ•°å­¦

## åŸºæœ¬æ•°æ®æ“ä½œ

å¼ é‡ï¼ˆ`tensor`ï¼‰æ˜¯PyTorchä¸­çš„åŸºæœ¬æ•°æ®ï¼Œå®ƒå¯ä»¥ç”¨äºè¡¨ç¤ºnç»´æ•°ç»„ã€‚

ä¸€ç»´å¼ é‡ç”¨äºè¡¨ç¤ºå‘é‡ï¼›äºŒç»´å¼ é‡ç”¨äºè¡¨ç¤ºçŸ©é˜µï¼›ä¸‰ç»´åŠä»¥ä¸Šæ²¡æœ‰æ•°å­¦å¯¹åº”ã€‚

```python
import torch

# å®šä¹‰äº†ä¸€ä¸ª3è¡Œ4åˆ—çš„çŸ©é˜µï¼Œå…¶ä¸­çš„å…ƒç´ æŒ‰é¡ºåºä¸€æ¬¡ä»0å¼€å§‹åˆ°11
X = torch.arange(12).reshape(3, 4)

# å¯ä»¥é€šè¿‡æ•°ç»„ä¼ å…¥å¼ é‡ä¸­çš„æ•°æ®
Y = torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])

# å¯ä»¥å¯¹åº”å…ƒç´ é—´åšè¿ç®—
X + Y
X > Y  # è¿™æ ·ä¼šå½¢æˆä¸€ä¸ªç›¸åŒå½¢çŠ¶çš„boolå¼ é‡

# å¯ä»¥å°†ä¸¤ä¸ªå¼ é‡æ‹¼æ¥
# äºŒç»´æƒ…å†µä¸‹ï¼šdim=0è¡¨ç¤ºæŒ‰è¡Œæ‹¼æ¥ï¼ˆå¾€ä¸‹æ‰©å±•ï¼‰ï¼Œdim=1è¡¨ç¤ºæŒ‰åˆ—æ‹¼æ¥ï¼ˆå¾€å³æ‰©å±•ï¼‰
torch.cat((X, Y), dim=0)
```

### æ•°æ®å¹¿æ’­

å½“æœ‰ä¸¤ä¸ªä¸åŒå½¢çŠ¶çš„å¼ é‡å‘ç”Ÿå¯¹åº”å…ƒç´ åšè®¡ç®—æ—¶ï¼Œæœ‰å¦‚ä¸‹æ•°æ®å¹¿æ’­è§„åˆ™ï¼š

1. `å¼ é‡X`å’Œ`å¼ é‡Y`æ‹¥æœ‰ä¸åŒçš„å½¢çŠ¶ï¼ˆå³å¼ é‡çš„ç»´åº¦æ•°ç›¸åŒï¼Œä½†å­˜åœ¨è‡³å°‘æŸä¸€ç»´åº¦çš„å‘é‡é•¿åº¦ä¸åŒï¼‰ï¼Œ
   å¿…é¡»æœ‰ä¸€ä¸ªå¼ é‡åœ¨å‘é‡é•¿åº¦ä¸åŒçš„ç»´åº¦å…¶é•¿åº¦ä¸º1ï¼ˆsingleton dimensionï¼‰ï¼Œè¿™æ ·å°±å¯ä»¥å°†è¯¥å¼ é‡çš„å‘é‡**æ‹·è´ä¸ºå¦ä¸€ä¸ªå¼ é‡çš„è¯¥ç»´åº¦é•¿åº¦ä»½**ç”¨ä»¥è®¡ç®—ã€‚
2. æ‹·è´å®Œæˆåå³å¯å®Œæˆ**æŒ‰å…ƒç´ è®¡ç®—**

```python
import torch

a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
print(a, "\n", b)
print(a + b)
```

è¿è¡Œç»“æœï¼š

```text
tensor([[0],
        [1],
        [2]])
tensor([[0, 1]])
tensor([[0, 1],
        [1, 2],
        [2, 3]])
```

### æ•°æ®é¢„å¤„ç†

```python
import panda as pd


def preprocess_read():
    ## å†™å…¥ä¸€ä¸ªcsvæ–‡ä»¶ï¼ˆæ•°æ®é›†ï¼‰
    def write_csv(data_file: str):
        os.makedirs(os.path.dirname(data_file), exist_ok=True)
        with open(data_file, "w") as f:
            ## æ³¨ï¼šæ•°æ®ä¸­æœ‰NaNæ•°æ®
            f.write(
                """NumRooms,RoofType,Price
NA,NA,127500
2,NA,106000
4,Slate,178100
NA,NA,140000
6,Poop,114514"""
            )

    write_csv(data_file)
    ## å¯¼å…¥csvæ–‡ä»¶æ•°æ®
    data = pd.read_csv(data_file)

    ## å®ä¾‹ç”¨æ³•ï¼šç»Ÿè®¡å‡ºæœ€å¤šNaNçš„åˆ—å¹¶åˆ é™¤è¿™ä¸€åˆ—
    def delete_largest_amount_of_nan_col():
        count = 0
        count_max = 0
        labels = ["NumRooms", "RoofType", "Price"]
        flag = labels[0]
        for label in labels:
            ## isnaåˆ¤æ–­è¯¥åˆ—æ¯ä¸ªå…ƒç´ æ˜¯å¦ä¸ºnan
            count = data[label].isna().sum()
            if count > count_max:
                count = count_max
                flag = label
        data_new = data.drop(flag, axis=1)
        print(torch.tensor(data_new.to_numpy(dtype=float)))

    print(data)
    delete_largest_amount_of_nan_col()

    ## ilocå¯ä»¥ç”¨äºç±»ä¼¼tensorçš„ç´¢å¼•æ–¹å¼å°†pandaä¸­çš„æ•°æ®åˆ‡ç‰‡æ‹¿å‡ºæ¥
    inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
    print(inputs)
    ## get_dummies ä¼šå°†å­—ç¬¦ä¸²çš„åˆ—RoofTypeå˜ä¸ºRoofType_Slateã€RoofType_Poopã€RoofType_nanä¸‰åˆ—ï¼Œå¹¶å°†å…¶ä¸­çš„å€¼å˜ä¸ºTrue/False
    ## å‘ˆç°çš„æ•ˆæœç±»ä¼¼one-hotè¡¨ç¤ºæ³•
    inputs = pd.get_dummies(inputs, dummy_na=True)
    print(inputs)
    ## ç»“æœå¦‚ä¸‹ï¼š
    #   NumRooms  RoofType_Poop  RoofType_Slate  RoofType_nan
    # 0       NaN          False           False          True
    # 1       2.0          False           False          True
    # 2       4.0          False            True         False
    # 3       NaN          False           False          True
    # 4       6.0           True           False         False
    ## è¿™ä¸ªæ–¹æ³•ä¼šå°†æ•°å­—ä¸­çš„nanæ”¹ä¸ºæŒ‡å®šå€¼
    inputs = inputs.fillna(inputs.mean())
    print(inputs)

    ## å¯ä»¥ç”¨to_numpyå°†pandaæ•°æ®è½¬æ¢ä¸ºå¯ä»¥ä¸ºPyTorchä½¿ç”¨çš„æ•°æ®
    X = torch.tensor(inputs.to_numpy(dtype=float))
    y = torch.tensor(outputs.to_numpy(dtype=float))
    print(X, "\n", y)


tensor_broadcast()
```

> [!NOTE]
> å¦‚æœ`pd.get_dummies(..., dummy_na=False)`ï¼Œåˆ™ä¸ä¼šæœ‰`RoofType_nan`è¿™ä¸€åˆ—

## çº¿æ€§ä»£æ•°

å‰é¢æåˆ°äº†å¼ é‡å¯ä»¥ç”¨äºè¡¨è¾¾æ•°å­¦ä¸­çš„å‘é‡å’ŒçŸ©é˜µï¼Œä¸‹é¢æˆ‘ä»¬çœ‹çœ‹é™¤äº†æŒ‰å…ƒç´ è¿ç®—ä»¥å¤–çš„çº¿æ€§ä»£æ•°è¿ç®—è¯¥æ€ä¹ˆå®ç°ã€‚

### é™ç»´

å¼ é‡é€šè¿‡`sum`å¯ä»¥å®ç°é™ç»´ï¼Œå°†æŸä¸€ä¸ªæˆ–å‡ ä¸ªç»´åº¦çš„æ•°æ®å‹ç¼©åˆ°ä¸€ä¸ªå…ƒç´ ä¸Š

```python
A = torch.arange(6, dtype=torch.float32).reshape(2, 3)
## Açš„å®šä¹‰å¦‚ä¸‹ï¼š
# tensor([[0., 1., 2.],
#         [3., 4., 5.]])
print(A.sum())  # ä»2ç»´é™åˆ°0ç»´æ ‡é‡
print(A.sum(dim=0))  # è¡Œå‹ç¼©
## ç»“æœå¦‚ä¸‹ï¼š
# tensor([3., 5., 7.])
print(A.sum(dim=1))  # åˆ—å‹ç¼©
## ç»“æœå¦‚ä¸‹ï¼š
# tensor([ 3., 12.])
```

::: details
æ—§ç‰ˆæœ¬ä¼ å…¥çš„æ˜¯`axis`ï¼Œæ–°ç‰ˆæœ¬å°†è¿™ä¸ªå‚æ•°æ”¹ä¸ºäº†`dim`

ï¼ˆä½†æš‚æ—¶è¿˜å…¼å®¹æ—§ç‰ˆæœ¬ï¼Œå¯ä»¥ä½¿ç”¨`axis`ï¼Œåªæ˜¯LSPä¼šæŠ¥é”™ï¼‰
:::

#### éé™ç»´æ±‚å’Œ

éé™ç»´æ±‚å’Œå¯ä»¥è®©æ•°æ®ä¿ç•™ç»´åº¦ä¿¡æ¯

```python
print(A.sum(dim=1, keepdims=True))
## ç»“æœå¦‚ä¸‹ï¼š
# tensor([[ 3.],
#         [12.]])
```

> [!NOTE]
> å¼ é‡å’Œå‘é‡ã€çŸ©é˜µæœ‰ä¸€ç‚¹ä¸å¤ªä¸€è‡´ï¼šæ•°å­¦ä¸Šå‘é‡å’ŒçŸ©é˜µçš„ç»´åº¦å¯ä»¥è§†ä¸ºç›¸åŒçš„ï¼Œéƒ½æ˜¯äºŒç»´ï¼ˆåªä¸è¿‡å‘é‡åœ¨å…¶ä¸­ä¸€ä¸ªç»´åº¦ä¸Šçš„é•¿åº¦ä¸º1ï¼‰ï¼Œæ‰€ä»¥åªè¦æ»¡è¶³è¦æ±‚å‘é‡å’ŒçŸ©é˜µæ˜¯å¯ä»¥åšç‚¹ä¹˜è¿ç®—çš„
>
> è€Œå¼ é‡æ—¢å¯å®šä¹‰çš„åªæœ‰ä¸€ä¸ªç»´åº¦çš„å‘é‡ï¼Œä¹Ÿå¯ä»¥å®šä¹‰æœ‰ä¸¤ä¸ªç»´åº¦çš„å‘é‡ã€‚å› æ­¤å®šä¹‰ä¸ºä¸¤ä¸ªç»´åº¦çš„å‘é‡ä¸çŸ©é˜µç›´æ¥ç‚¹ä¹˜è¿ç®—çš„æ–¹æ³•åå’Œä¸€ä¸ªç»´åº¦çš„å‘é‡çš„æ–¹æ³•åæœ‰æ‰€ä¸åŒ

### çŸ©é˜µä¹˜æ³•

```python
# å‘é‡ç‚¹ç§¯
x = torch.arange(3, dtype=torch.float32)
y = torch.ones(3, dtype=torch.float32)
print((x, y, torch.dot(x, y)))
# (tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))
# çŸ©é˜µä¹˜æ³• - çŸ©é˜µå’Œå‘é‡ï¼ˆä¸€ç»´å¼ é‡ï¼‰ä¹˜
print((A.shape, x.shape, torch.mv(A, x), A @ x))
# (torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))
# çŸ©é˜µä¹˜æ³• - çŸ©é˜µå’Œå‘é‡ï¼ˆäºŒç»´å¼ é‡ï¼‰ä¹˜
x.reshape(3, 1)
print((A.shape, x.shape, torch.mm(A, x), A @ x))
# ç»“æœä¸ºï¼š
# (torch.Size([2, 3]),
# torch.Size([3, 1]),
# tensor([[ 5.],
#         [14.]]),
# tensor([[ 5.],
#         [14.]]))
# çŸ©é˜µä¹˜æ³• - çŸ©é˜µå’ŒçŸ©é˜µä¹˜
B = torch.ones(3, 4)
print(A, B, (torch.mm(A, B), A @ B))
# ç»“æœä¸ºï¼š
# (tensor([[0., 1., 2.],
#         [3., 4., 5.]]),
# tensor([[1., 1., 1., 1.],
#         [1., 1., 1., 1.],
#         [1., 1., 1., 1.]]),
# tensor([[ 3.,  3.,  3.,  3.],
#         [12., 12., 12., 12.]]),
# tensor([[ 3.,  3.,  3.,  3.],
#         [12., 12., 12., 12.]]))
```

> [!NOTE]
> æ–¹æ³•`torch.ones()`é»˜è®¤è¿”å›çš„å°±æ˜¯float32çš„æ•°æ®

---

> [!TIP]
> æ˜¾ç„¶ç›´æ¥ä½¿ç”¨`A @ B`è¿™æ ·çš„å†™æ³•æ›´åŠ çœå¿ƒ
>
> è€Œè¿™ç§å†™æ³•æ˜¾ç„¶æ˜¯ç”¨äº†è¿ç®—ç¬¦é‡è½½å®ç°çš„ï¼Œåªè¦åœ¨ç±»å†…å®ç°äº†`__matmul__`æ—¢å¯å®ŒæˆçŸ©é˜µä¹˜æ³•è¿ç®—
>
> [ç›¸å…³çš„Pythonå®˜æ–¹æ–‡æ¡£](https://docs.python.org/3/whatsnew/3.5.html#pep-465-a-dedicated-infix-operator-for-matrix-multiplication)

### èŒƒæ•°

æœ€åˆæ˜¯é’ˆå¯¹å‘é‡æå‡ºçš„æ¦‚å¿µï¼Œç”¨äºè¡¨ç¤ºä¸€ä¸ªå‘é‡çš„å¤§å°ï¼ˆä¸æ˜¯ç»´åº¦ä¸ªæ•°ï¼‰ã€‚

å‘é‡èŒƒæ•°æ˜¯å°†å‘é‡æ˜ å°„åˆ°æ ‡é‡çš„å‡½æ•°$f$ï¼Œå‘é‡èŒƒæ•°å¿…é¡»æ»¡è¶³ä¸‹é¢ä¸‰ä¸ªè¦æ±‚ï¼š

$$ f(\alpha \mathbf{x}) = |\alpha|f(\mathbf{x}) $$

$$ f(\mathbf{x + y}) \le f(\mathbf{x}) + f(\mathbf{y}) $$

$$ f(\mathbf{x}) \ge 0 $$

å¸¸è§çš„èŒƒæ•°ï¼š

- $L_p$èŒƒæ•°ï¼š

  $$ \|\mathbf{x}\|_p = (\sum_{i=1}^n |x_i|^p)^\frac{1}{p} $$

> [!TIP]
> å¸¸è§çš„$L_1$èŒƒæ•°å’Œ$L_2$èŒƒæ•°éƒ½æ˜¯è¿™ä¸ª$L_p$èŒƒæ•°çš„ç‰¹æ®Šæƒ…å†µï¼ˆ$p=\{1,2\}$ï¼‰

---

> [!NOTE]
> å¸¸è§çš„èŒƒæ•°ä¸€èˆ¬éƒ½æŒ‡$L_2$èŒƒæ•°ï¼Œé€šå¸¸å®ƒéƒ½ä¼šçœç•¥ä¸‹æ ‡ï¼š$\|\mathbf{x}\|$ç­‰ä»·äº$\|\mathbf{x}\|_2$
>
> å‘é‡å¯ä»¥è¿™æ ·æ±‚$L_2$èŒƒæ•°ï¼š
>
> ```python
> u = torch.tensor([3.0, -4.0])
> torch.norm(u)  # 5
> ```
>
> æƒ³æ±‚$L_1$èŒƒæ•°æ¯”è¾ƒç®€å•ï¼Œå› ä¸ºå®ƒå°±æ˜¯æ±‚å„ä¸ªåˆ†é‡ç»å¯¹å€¼çš„å’Œï¼š
>
> ```python
> torch.abs(u).sum()
> ```

- çŸ©é˜µçš„FrobeniusèŒƒæ•°ï¼š
  $$
  \|\mathbf{x}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}
  $$

> [!NOTE]
> å®ƒå’Œå‘é‡$L_2$èŒƒæ•°å¾ˆç±»ä¼¼ï¼Œæ‰€ä»¥åœ¨PyTorchä¸­è·å¾—çŸ©é˜µèŒƒæ•°çš„æ–¹æ³•ä¹Ÿå’Œè·å–å‘é‡èŒƒæ•°çš„æ–¹æ³•å¾ˆç±»ä¼¼ï¼š
>
> ```python
> torch.norm(torch.ones((4, 9)))  # 6
> ```

---

> [!TIP]
> å°†çŸ©é˜µèŒƒæ•°è¿›ä¸€æ­¥æ¨å¹¿åˆ°å¼ é‡ä¸Šï¼Œå…¶å®å°±å¯ä»¥å¾—åˆ°nç»´å¼ é‡çš„èŒƒæ•°ï¼š
>
> $$
> \|\mathbf{x}\|_F = \sqrt{\sum_{i_1 =1}^{l_1} \sum_{i_2 =1}^{l_2} ...
>                            \sum_{i_n =1}^{l_n} x_{i_1 i_2...i_n}^2}
> $$
>
> PyTorchä¸­å¯¹ä»»æ„ç»´åº¦çš„å¼ é‡çš„è®¡ç®—ç»“æœä¹Ÿæ»¡è¶³ä¸Šå¼
>
> ```python
> A = torch.ones(2, 2, 4)
> print(A.norm())  # 4
> ```

## å¾®ç§¯åˆ†

å¾®ç§¯åˆ†åŸºæœ¬æ¦‚å¿µè®²å¾—å¾ˆæµ…ï¼Œè¿™äº›ä¸œè¥¿æ—©å°±åœ¨æ•°å­¦è¯¾ä¸Šå­¦å¾—æ»šç“œçƒ‚ç†Ÿäº†

ä¸»è¦å…³æ³¨æ¢¯åº¦çš„ä¸€ä¸ªç»“è®ºå’Œè‡ªåŠ¨å¾®åˆ†çš„åŸç†ï¼š

### æ¢¯åº¦

æ¢¯åº¦å®šä¹‰ï¼š

$$
\nabla_\mathbf{x} f(\mathbf{x}) = [\frac{\partial f(\mathbf{x})}{\partial x_1},
                                     \frac{\partial f(\mathbf{x})}{\partial x_2},
                                 ... \frac{\partial f(\mathbf{x})}{\partial x_n}]^T
$$

å‡è®¾$\mathbf{x}$ä¸ºnç»´å‘é‡ï¼Œæœ‰ä»¥ä¸‹æ¢¯åº¦è®¡ç®—ç»“è®ºï¼š

- å¯¹äºæ‰€æœ‰$A \in \mathbb{R}^{m\times n}$ï¼Œéƒ½æœ‰$\nabla_\mathbf{x} \mathbf{Ax} = \mathbf{A^T}$
- å¯¹äºæ‰€æœ‰$A \in \mathbb{R}^{m\times n}$ï¼Œéƒ½æœ‰$\nabla_\mathbf{x} \mathbf{x^T A} = \mathbf{A}$
- å¯¹äºæ‰€æœ‰$A \in \mathbb{R}^{m\times n}$ï¼Œéƒ½æœ‰
  $\nabla_\mathbf{x} \mathbf{x^T Ax} = \mathbf{(A+A^T)x}$
- $\nabla_\mathbf{x} \|\mathbf{x}\|^2 = \nabla_\mathbf{x} \mathbf{x^T x} = \mathbf{2x}$

> [!NOTE]
> è¯¥ä¹¦ä¸­ä»‹ç»çš„æ¢¯åº¦$\nabla_\mathbf{x} \mathbf{Ax}$ä¸é›…å¯æ¯”çŸ©é˜µæ°å¥½äº’ä¸ºè½¬ç½®ã€‚è¿™åªæ˜¯è®°æ³•ä¸åŒç½¢äº†ï¼Œæ— é¡»æ·±ç©¶
>
> å¦å¤–ï¼Œåˆå­¦æ—¶å¯èƒ½å¯¹å‘é‡$\mathbf{y}$å¯¹å‘é‡$\mathbf{x}$æ±‚å¯¼çš„æ¦‚å¿µä¸æ¸…æ™°ï¼Œä¼šè§‰å¾—æ¢¯åº¦è¿ç®—å¯ä»¥ç›´æ¥é€ç©¿åˆ°çŸ©é˜µå†…å¯¹æ¯ä¸ªæ ‡é‡å®Œæˆè®¡ç®—å³å¯ï¼Œè¿™æ ·ä¼šå¯¼è‡´å‡ºç°ä¸€ä¸ªè¶…é•¿å‘é‡çš„å‡ºç°ï¼Œå¯¹äºçŸ©é˜µæ¢¯åº¦çš„è®¡ç®—ç»“è®ºè®¾è®¡ä¸åˆ©ï¼ˆå°†ä¼šä¸¢åœ¨æ ‡é‡ä¸Šçš„æ±‚å¯¼è®¡ç®—è§„å¾‹ï¼‰ï¼Œæ•°å­¦ä¸Šå¹¶æ²¡æœ‰é‡‡ç”¨è¿™ç§æ–¹å¼

### è‡ªåŠ¨å¾®åˆ†

PyTorchæ”¯æŒé€šè¿‡**åå‘ä¼ æ’­**è®¡ç®—ä¸€ä¸ªæ ‡é‡$y$å¯¹å‘é‡$\mathbf{x}$çš„æ¢¯åº¦ã€‚

:::details
åå‘ä¼ æ’­ç®—æ³•ï¼šæˆ‘å¾ˆå–œæ¬¢ã€Šæ·±åº¦å­¦ä¹ å…¥é—¨ï¼šåŸºäºPythonçš„ç†è®ºä¸å®ç°ã€‹ä¸­ç»™å‡ºçš„è®¡ç®—å›¾æŠ½è±¡æ¥ç†è§£è¿™ä¸ªæ¦‚å¿µ

å¦‚æœä½ ä¸çŸ¥é“åå‘ä¼ æ’­ï¼Œæ¨èå»çœ‹çœ‹è¿™æœ¬ä¹¦ç¬¬äº”ç« 

![åå‘ä¼ æ’­ç¤ºæ„å›¾](../pic/deep_learning/chapter2_1.png)

![åå‘ä¼ æ’­ç¤ºæ„å›¾](../pic/deep_learning/chapter2_2.png)
:::

```python
import torch

x = torch.arange(4.0, requires_grad=True)  # å°†xæ ‡è®°ä¸ºæ±‚æ¢¯åº¦çš„è‡ªå˜é‡
print(x.grad)  # åˆå§‹å€¼ä¸ºNone
y = 2 * torch.dot(x, x)  # y = 2x^T*x ï¼ˆå‡è®¾xæ˜¯åˆ—å‘é‡ï¼‰
y.backward()  # y'= 4x     ï¼ˆç±»ä¼¼xæ˜¯æ ‡é‡æ—¶çš„y=2x^2çš„æ±‚å¯¼ï¼‰
print(x.grad)  # tensor([0., 4., 8., 12.])

# åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorchä¼šç´¯ç§¯æ¢¯åº¦ï¼Œæˆ‘ä»¬éœ€è¦æ¸…é™¤ä¹‹å‰çš„å€¼
x.grad.zero_()
y = x.sum()
y.backward()
print(x.grad)  # tensor([1., 1., 1., 1.])
```

> [!WARNING]
> å¤šæ¬¡`backward`é»˜è®¤ä¼šåœ¨è‡ªå˜é‡ä¸Šç´¯è®¡æ¢¯åº¦ï¼Œå¦‚æœä¸éœ€è¦ä¸€å®šè®°å¾—ç”¨`grad.zero_()`æ¸…é™¤

#### éæ ‡é‡è¾“å‡ºå˜é‡çš„åå‘ä¼ æ’­

æ•°å­¦ä¸Šå‘é‡$\mathbf{y}$å¯¹å‘é‡$\mathbf{x}$æ±‚å¯¼ï¼Œç»“æœåº”è¯¥æ˜¯é›…å¯æ¯”çŸ©é˜µ$J$

$$
J = (\frac{\mathbf{\partial y}}{\partial x_1} ...
\frac{\mathbf{\partial y}}{\partial x_n}) = \begin{pmatrix}
\frac{\partial y_1}{\partial x_1}&\cdots &\frac{\partial y_1}{\partial x_n}\\
\vdots &\ddots &\vdots \\
\frac{\partial y_m}{\partial x_1}&\dots &\frac{\partial y_m}{\partial x_n}
\end{pmatrix}
$$

ä½†æ˜¯PyTorchæ±‚è§£æ¢¯åº¦çš„ç›®çš„ä¸æ˜¯çº¯ç²¹çš„æ•°å­¦æ¨å¯¼ï¼Œè€Œæ˜¯å®Œæˆç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­è®¡ç®—ï¼Œå› æ­¤PyTorchçš„`torch.autograd`éœ€è¦ä¸€ä¸ªæ ‡é‡$l$ä½œä¸ºçœŸæ­£çš„æœ€ç»ˆç»“æœ

åœ¨è¾“å‡ºä¸ºå‘é‡$\mathbf{y}$çš„æƒ…å†µä¸‹ï¼Œè¿˜éœ€è¦ä¸€ä¸ªå‘é‡$\mathbf{v}$ä½œä¸ºæ ‡é‡å‡½æ•°$l=g(\mathbf{y})$çš„æ¢¯åº¦ï¼š

$$
\mathbf{v}=\begin{pmatrix}\frac{\partial l}{\partial y_1} ...
\frac{\partial l}{\partial y_m}\end{pmatrix}^T
$$

åˆ™æ ¹æ®é“¾å¼æ³•åˆ™

$$
\frac{\partial z}{\partial \mathbf{x}}=
(\frac{\partial \mathbf{y}}{\partial \mathbf{x}})^T \frac{\partial z}{\partial \mathbf{y}}
$$

`torch.autograd`æœ€ç»ˆè®¡ç®—çš„æ˜¯ä»¥$\mathbf{x}$ä¸ºè‡ªå˜é‡çš„æ ‡é‡$l$çš„æ¢¯åº¦ï¼š

$$
J^T\cdot \mathbf{v} =
\begin{pmatrix}
\frac{\partial y_1}{\partial x_1}&\cdots &\frac{\partial y_m}{\partial x_1}\\
\vdots &\ddots &\vdots \\
\frac{\partial y_1}{\partial x_n}&\dots &\frac{\partial y_m}{\partial x_n}
\end{pmatrix}
\begin{pmatrix}
\frac{\partial l}{\partial y_1}\\
\vdots\\
\frac{\partial l}{\partial y_m}
\end{pmatrix}
= \begin{pmatrix}
\frac{\partial l}{\partial x_1}\\
\vdots\\
\frac{\partial l}{\partial x_n}
\end{pmatrix}
$$

ç»¼ä¸Šï¼Œ`torch.autograd`æ˜¯è®¡ç®—**å‘é‡é›…å¯æ¯”ç‚¹ä¹˜**çš„æ¡†æ¶ï¼ˆè€Œä¸æ˜¯ç›´æ¥ç®—é›…å¯æ¯”çŸ©é˜µçš„æ¡†æ¶ï¼‰

å®é™…ä½¿ç”¨æ—¶çš„ä»£ç å¾€å¾€æ˜¯æ‰¹å¤„ç†è®­ç»ƒ(batch)æ—¶éœ€è¦è®¡ç®—æ¯ä¸€ä¸ªæ ‡é‡yå„è‡ªçš„æ¢¯åº¦çš„å’Œï¼Œåˆ™å¯ä»¥è¿™æ ·å†™ï¼š

```python
import torch

x = torch.arange(4.0, requires_grad=True)  # å°†xæ ‡è®°ä¸ºæ±‚æ¢¯åº¦çš„è‡ªå˜é‡
y = x * x
# è¿™é‡ŒæŒ‡å®šçš„gradientå°±æ˜¯å‘é‡vï¼Œå®šä¹‰çš„l=y1+...+ynï¼ˆæ‰¹å¤„ç†æŠŠæ¯ä¸ªè¾“å…¥å¾—åˆ°çš„yåŠ åœ¨ä¸€èµ·äº†ï¼‰
y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()
x.grad
```

#### åˆ†ç¦»è®¡ç®—

æœ‰æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æŸäº›è®¡ç®—ç§»åŠ¨åˆ°è®°å½•çš„è®¡ç®—å›¾ä¹‹å¤–ã€‚ ä¾‹å¦‚ï¼Œå‡è®¾yæ˜¯ä½œä¸ºxçš„å‡½æ•°è®¡ç®—çš„ï¼Œè€Œzåˆ™æ˜¯ä½œä¸ºyå’Œxçš„å‡½æ•°è®¡ç®—çš„ã€‚
æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬æƒ³è®¡ç®—zå…³äºxçš„æ¢¯åº¦ï¼Œä½†ç”±äºæŸç§åŸå› ï¼Œå¸Œæœ›å°†yè§†ä¸ºä¸€ä¸ªå¸¸æ•°ï¼Œ å¹¶ä¸”åªè€ƒè™‘åˆ°xåœ¨yè¢«è®¡ç®—åå‘æŒ¥çš„ä½œç”¨ã€‚

è¿™é‡Œå¯ä»¥åˆ†ç¦»yæ¥è¿”å›ä¸€ä¸ªæ–°å˜é‡uï¼Œè¯¥å˜é‡ä¸yå…·æœ‰ç›¸åŒçš„å€¼ï¼Œ ä½†ä¸¢å¼ƒè®¡ç®—å›¾ä¸­å¦‚ä½•è®¡ç®—yçš„ä»»ä½•ä¿¡æ¯ã€‚
æ¢å¥è¯è¯´ï¼Œæ¢¯åº¦ä¸ä¼šå‘åæµç»uåˆ°xã€‚
å› æ­¤ï¼Œä¸‹é¢çš„åå‘ä¼ æ’­å‡½æ•°è®¡ç®—z=u*xå…³äºxçš„åå¯¼æ•°ï¼ŒåŒæ—¶å°†uä½œä¸ºå¸¸æ•°å¤„ç†ï¼Œ
è€Œä¸æ˜¯z=x*x\*xå…³äºxçš„åå¯¼æ•°ã€‚

```python
import torch

x = torch.arange(4.0, requires_grad=True)  # å°†xæ ‡è®°ä¸ºæ±‚æ¢¯åº¦çš„è‡ªå˜é‡
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
print(x.grad == u)  # tensor([True, True, True, True])

x.grad.zero_()
y.sum().backward()
print(x.grad == 2 * x)  # tensor([True, True, True, True])
```

> [!NOTE]
> éšåå†æ¬¡backwordå°±å¯ä»¥å¾—åˆ°yåˆ°xçš„æ¢¯åº¦ä¿¡æ¯ï¼ˆå»é™¤äº†zçš„å¦ä¸€æ¡åˆ†æ”¯ï¼‰

#### å¸¦pythonæ§åˆ¶æµçš„æ¢¯åº¦è®¡ç®—ï¼ˆåŠ¨æ€åå‘ä¼ æ’­å›¾ï¼‰

```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c


a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()
# d=f(a)æ˜¯åˆ†æ®µçº¿æ€§å‡½æ•°ï¼Œæ‰€ä»¥ä»»æ„ä½ç½®çš„å¾®åˆ†åº”è¯¥éƒ½å’Œd / aç›¸ç­‰
print(a.grad == d / a)  # tensor(True)
```

## æ¦‚ç‡è®º

æ¦‚ç‡è®ºåŸºæœ¬æ¦‚å¿µæœ‰ä¸€ä¸ªæ¯”è¾ƒç‰¹æ®Šçš„æ²¡å­¦è¿‡çš„æ¦‚å¿µå¯ä»¥çœ‹çœ‹ï¼š[å¤šé¡¹åˆ†å¸ƒ](https://math.fandom.com/zh/wiki/%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83)

```python
import torch
from torch.distributions.multinomial import Multinomial
import matplotlib.pyplot as plt

# æŠ•ğŸ²ï¼šå„ä¸ªé¢æ¦‚ç‡ç›¸ç­‰ï¼Œç”¨å¤šæ¬¡æŠ•å‡ºçš„ç»“æœç»Ÿè®¡é¢‘ç‡ä¼šè¶‹è¿‘äºæ¦‚ç‡
fair_probs = torch.ones(6) / 6
print("æ¦‚ç‡ï¼š", fair_probs)
counts = Multinomial(1000, fair_probs).sample()
print(counts)
print("é¢‘ç‡ï¼š", counts / 1000)

# å¯ä»¥å¯è§†åŒ–çœ‹çœ‹éšç€æŠ•å‡ºæ¬¡æ•°å¢é•¿é¢‘ç‡çš„å˜åŒ–æƒ…å†µ
counts = Multinomial(10, fair_probs).sample((500,))
cum_counts = counts.cumsum(dim=0)
estimates = cum_counts / cum_counts.sum(dim=1, keepdim=True)

for i in range(6):
    plt.plot(estimates[:, i].numpy(), label=("P(die=" + str(i + 1) + ")"))
plt.axhline(y=0.167, color="black", linestyle="dashed")
plt.gca().set_xlabel("Groups of experiments")
plt.gca().set_ylabel("Estimated probability")
plt.legend()
plt.show()
```

ç»“æœä¼šç±»ä¼¼ä¸‹é¢è¿™å¼ å›¾ï¼š
![é¢‘ç‡ä¼°è®¡æ¦‚ç‡](https://zh.d2l.ai/_images/output_probability_245b7d_63_0.svg)
